<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AI-Fundamental</title>
  
  <subtitle>Learning AI is not so hard.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ai-fundamental.github.io/"/>
  <updated>2018-11-29T03:17:52.351Z</updated>
  <id>https://ai-fundamental.github.io/</id>
  
  <author>
    <name>Ping Luo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>The knowledge graph of the third course part: Learning</title>
    <link href="https://ai-fundamental.github.io/2018/The%20knowledge%20graph%20of%20the%20third%20course%20part%20Learning/"/>
    <id>https://ai-fundamental.github.io/2018/The knowledge graph of the third course part Learning/</id>
    <published>2018-11-30T08:00:00.000Z</published>
    <updated>2018-11-29T03:17:52.351Z</updated>
    
    <content type="html"><![CDATA[<p>To view the whole knowledge graph.</p><ul><li>click <a href="/assets/机器学习课程.svg">here</a> for Chinese version.</li><li>click <a href="/assets/Machine_Learning_Course.svg">here</a> for English version.</li></ul><p><a href="/assets/Machine_Learning_Course.png"><img src="/assets/Machine_Learning_Course.png" alt="TestSVG"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;To view the whole knowledge graph.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;click &lt;a href=&quot;/assets/机器学习课程.svg&quot;&gt;here&lt;/a&gt; for Chinese version.&lt;/li&gt;
&lt;li&gt;click &lt;a href=&quot;
      
    
    </summary>
    
    
      <category term="knowledge graph" scheme="https://ai-fundamental.github.io/tags/knowledge-graph/"/>
    
  </entry>
  
  <entry>
    <title>Programming 1 on Learning</title>
    <link href="https://ai-fundamental.github.io/2018/Programming%201%20on%20Learning/"/>
    <id>https://ai-fundamental.github.io/2018/Programming 1 on Learning/</id>
    <published>2018-10-19T08:00:00.000Z</published>
    <updated>2018-11-29T03:06:29.084Z</updated>
    
    <content type="html"><![CDATA[<ul><li>View on Pages (Directly click the title)</li><li>View on <a href="/assets/CDTensorflow.html">Jupyter Notebook</a>  </li><li>Download <a href="/assets/CDTensorflow.ipynb">Jupyter Notebook</a>  </li></ul><a id="more"></a><h3 id="Import-packages"><a href="#Import-packages" class="headerlink" title="Import packages"></a>Import packages</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"><span class="comment"># ============================================================================</span></div><div class="line"><span class="comment">#   Copyright (C) 2017 All rights reserved.</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#   filename : Logistic_Regression.py</span></div><div class="line"><span class="comment">#   author   : chendian / okcd00@qq.com</span></div><div class="line"><span class="comment">#   date     : 2018-09-26</span></div><div class="line"><span class="comment">#   desc     : Tensorflow Logistic Regression Tutorial</span></div><div class="line"><span class="comment">#   </span></div><div class="line"><span class="comment"># ============================================================================</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><span class="keyword">import</span> os</div><div class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">""</span></div><div class="line"><span class="comment"># jupyter notebook</span></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure><h3 id="Loading-training-data-from-sklearn"><a href="#Loading-training-data-from-sklearn" class="headerlink" title="Loading training data from sklearn"></a>Loading training data from sklearn</h3><blockquote><p>如果需要使用 sklearn 第三方库中自带的数据集，这里我列出了三种，方便调用与测试</p></blockquote><p><strong>sklearn的安装</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install sklearn</div></pre></td></tr></table></figure></p><p><strong>调用方法</strong></p><ul><li>data = load_data(name=’moons’)</li><li>data = load_data(name=’circles’)</li><li>data = load_data(name=’linear’)</li></ul><p>如果安装sklearn有困难，也可以直接从文件读取：</p><ul><li>data = load_data(name=’moons’, True)</li><li>data = load_data(name=’circles’, True)</li><li>data = load_data(name=’linear’, True)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># use data from sklearn package</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_moons</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</div><div class="line">    np.random.seed(<span class="number">0</span>)</div><div class="line">    X, y = make_moons(<span class="number">800</span>, noise=<span class="number">0.2</span>)</div><div class="line">    <span class="keyword">print</span> (<span class="string">"dataset shape:"</span>, X.shape)</div><div class="line">    </div><div class="line">    <span class="comment"># return train validate test sets </span></div><div class="line">    <span class="keyword">return</span> [(X[<span class="number">0</span>:<span class="number">600</span>,],y[<span class="number">0</span>:<span class="number">600</span>,]), (X[<span class="number">600</span>:<span class="number">800</span>,],y[<span class="number">600</span>:<span class="number">800</span>,])]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_circles</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_circles</div><div class="line">    np.random.seed(<span class="number">0</span>)</div><div class="line">    X, y = make_circles(<span class="number">800</span>, noise=<span class="number">0.2</span>, factor=<span class="number">0.5</span>, random_state=<span class="number">2</span>)</div><div class="line">    <span class="keyword">print</span> (<span class="string">"dataset shape:"</span>, X.shape)</div><div class="line">    </div><div class="line">    <span class="comment"># return train validate test sets </span></div><div class="line">    <span class="keyword">return</span> [(X[<span class="number">0</span>:<span class="number">600</span>,],y[<span class="number">0</span>:<span class="number">600</span>,]), (X[<span class="number">600</span>:<span class="number">800</span>,],y[<span class="number">600</span>:<span class="number">800</span>,])]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_linear</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</div><div class="line">    np.random.seed(<span class="number">0</span>)</div><div class="line">    X, y = make_classification(</div><div class="line">        <span class="number">800</span>, n_features=<span class="number">2</span>, n_redundant=<span class="number">0</span>, n_informative=<span class="number">1</span>,</div><div class="line">        random_state=<span class="number">1</span>, n_clusters_per_class=<span class="number">1</span>)</div><div class="line">    <span class="keyword">print</span> (<span class="string">"dataset shape:"</span>, X.shape)</div><div class="line">    </div><div class="line">    <span class="comment"># return train validate test sets </span></div><div class="line">    <span class="keyword">return</span> [(X[<span class="number">0</span>:<span class="number">600</span>,],y[<span class="number">0</span>:<span class="number">600</span>,]), (X[<span class="number">600</span>:<span class="number">800</span>,],y[<span class="number">600</span>:<span class="number">800</span>,])]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(name=<span class="string">'moons'</span>, load_directly=False)</span>:</span></div><div class="line">    _datasets=&#123;</div><div class="line">        <span class="string">'moons'</span>: load_moons,</div><div class="line">        <span class="string">'linear'</span>: load_linear,</div><div class="line">        <span class="string">'circles'</span>: load_circles,</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        ret = pickle.load(open(<span class="string">'./&#123;&#125;.pkl'</span>.format(name), <span class="string">'r'</span>)) <span class="keyword">if</span> load_directly <span class="keyword">else</span> _datasets[name]()</div><div class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</div><div class="line">        print(<span class="string">"set name as 'moons', 'linear' or 'circles',\n or check your files' existence"</span>)</div><div class="line">        print(e)</div><div class="line">    <span class="keyword">return</span> ret</div></pre></td></tr></table></figure><h3 id="Define-network"><a href="#Define-network" class="headerlink" title="Define network"></a>Define network</h3><blockquote><p>此处给出的是逻辑回归(Logistic Regression)的神经网络结构</p></blockquote><p>对于输入向量x，其属于类别i的概率为：<br>$$\begin{align<em>}<br>P\left ( Y=i\mid x,W,b \right ) &amp;=softmax_i\left ( Wx+b \right ) \<br> &amp;= \frac{e^{W_ix+b<em>i}}{\sum</em>{j}e^{W_jx+b_j}}<br>\end{align</em>}$$</p><p>模型对于输入向量x的预测结果y<em>pred是所有类别的预测中概率值最大的，即<br>$$y</em>{pred}=argmax_iP\left ( Y=i\mid x,W,b \right )$$</p><p>在LR模型中，需要求解的参数为权重矩阵W和偏置向量b，为了求解模型的两个参数，首先必须定义损失函数。对于上述的多类别Logistic回归，可以藉由Log似然函数作为其损失函数（<strong>负对数似然</strong> 注意取负）：<br>$$L\left ( \theta =\left { W,b \right },D \right )=\sum_{i=0}^{\left | D \right |}log\left ( P\left ( Y=y^{\left ( i \right )}\mid x^{\left ( i \right )},W,b  \right ) \right )$$</p><p>P.S. 代码中使用的<code>softmax_cross_entropy(y_truth, logits)</code>效果为:<br>先将<code>logits</code>作<code>softmax</code>操作获得<code>y_pred</code>，然后使用<code>y_truth</code>和<code>y_pred</code>作负对数似然。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_in, n_out)</span>:</span></div><div class="line">        self.X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_in], name=<span class="string">'X'</span>)</div><div class="line">        self.y = tf.placeholder(tf.int32, [<span class="keyword">None</span>], name=<span class="string">'y'</span>)</div><div class="line">        self.init_variables(n_in, n_out)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_variables</span><span class="params">(self, n_in, n_out)</span>:</span></div><div class="line">        <span class="comment"># n_in means n_features</span></div><div class="line">        <span class="comment"># n_out means n_classes</span></div><div class="line">        self.W = tf.Variable(</div><div class="line">            initial_value=tf.constant(<span class="number">0.0</span>, shape=[n_in, n_out]),</div><div class="line">            dtype=tf.float32, name=<span class="string">'weight'</span>)</div><div class="line">        self.b = tf.Variable(</div><div class="line">            initial_value=tf.constant(<span class="number">0.0</span>, shape=[n_out]),</div><div class="line">            dtype=tf.float32, name=<span class="string">'bias'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(self, logits)</span>:</span></div><div class="line">        <span class="comment"># softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)</span></div><div class="line">        <span class="keyword">return</span> tf.nn.softmax(logits, <span class="number">-1</span>)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">negative_log_likelihood</span><span class="params">(self, y_pred, y)</span>:</span></div><div class="line">        prob = self.sigmoid(y_pred)</div><div class="line">        positive_likelihood = tf.log(prob) * y.reshape(<span class="number">-1</span>, <span class="number">1</span>)</div><div class="line">        negative_likelihood = tf.log(<span class="number">1</span> - prob) * (<span class="number">1</span> - y.reshape(<span class="number">-1</span>, <span class="number">1</span>))</div><div class="line">        log_likelihood = positive_likelihood + negative_likelihood</div><div class="line">        <span class="keyword">return</span> -tf.reduce_mean(log_likelihood)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_network</span><span class="params">(self)</span>:</span></div><div class="line">        hidden = tf.matmul(self.X, self.W) + self.b</div><div class="line">        self.y_pred = tf.argmax(self.softmax(hidden), axis=<span class="number">-1</span>)  </div><div class="line">        <span class="keyword">return</span> self.y_pred, hidden</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_loss</span><span class="params">(self, hidden)</span>:</span></div><div class="line">        <span class="comment"># self.loss = self.negative_log_likelihood(y_pred, y)</span></div><div class="line">        <span class="comment"># self.loss = tf.nn.sigmoid_cross_entropy_with_logits(y_pred, y)</span></div><div class="line">        onehot_labels = tf.one_hot(self.y, depth=<span class="number">2</span>)</div><div class="line">        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=hidden)</div><div class="line">        <span class="keyword">return</span> tf.reduce_mean(self.loss)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen_input</span><span class="params">(self, data_x, data_y=None)</span>:</span></div><div class="line">        feed_dict = &#123;&#125;</div><div class="line">        feed_dict[self.X] = data_x</div><div class="line">        <span class="keyword">if</span> data_y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:    </div><div class="line">            self.y_truth = data_y</div><div class="line">            feed_dict[self.y] = data_y</div><div class="line">        <span class="keyword">return</span> feed_dict</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">errors</span><span class="params">(self, y_pred, y_truth=None)</span>:</span></div><div class="line">        <span class="keyword">if</span> y_truth <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            y_truth = self.y</div><div class="line">            not_equal_counts = tf.abs(y_pred - y_truth)</div><div class="line">            <span class="keyword">return</span> tf.reduce_mean(not_equal_counts)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            not_equal_counts = abs(y_pred - y_truth)</div><div class="line">            <span class="keyword">return</span> np.mean(not_equal_counts)</div></pre></td></tr></table></figure><h3 id="Define-optimizer"><a href="#Define-optimizer" class="headerlink" title="Define optimizer"></a>Define optimizer</h3><blockquote><p>因为深度学习常见的是对于梯度的优化，也就是说，<br>优化器最后其实就是各种对于梯度下降算法的优化。</p></blockquote><p>常见的优化器有 SGD，RMSprop，Adagrad，Adadelta，Adam 等，<br>此处实例中使用的是随机梯度下降（Stochastic gradient descent），<br>因为大多数机器学习任务就是最小化损失，在损失定义的情况下，后面的工作就交给优化器处理即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_optimization</span><span class="params">(datasets, learning_rate=<span class="number">0.10</span>, n_epochs=<span class="number">50</span>, draw_freq=<span class="number">10</span>)</span>:</span></div><div class="line">    train_set_x, train_set_y = datasets[<span class="number">0</span>]</div><div class="line">    test_set_x,  test_set_y  = datasets[<span class="number">1</span>]</div><div class="line">    classifier = LogisticRegression(n_in=<span class="number">2</span>, n_out=<span class="number">2</span>)  <span class="comment"># Classifier</span></div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_model_train</span><span class="params">()</span>:</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">            y_pred, hidden = classifier.get_network()</div><div class="line">            loss = classifier.get_loss(hidden)</div><div class="line">            <span class="keyword">return</span> y_pred, loss</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_model_test</span><span class="params">()</span>:</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'test'</span>):</div><div class="line">            y_pred, hidden = classifier.get_network()</div><div class="line">            <span class="keyword">return</span> y_pred</div><div class="line">        </div><div class="line">    train_output = get_model_train()  <span class="comment"># y_pred, loss</span></div><div class="line">    test_output = get_model_test()  <span class="comment"># y_pred</span></div><div class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(train_output[<span class="number">-1</span>])</div><div class="line">    </div><div class="line">    init = tf.global_variables_initializer()</div><div class="line">    sess = tf.Session()</div><div class="line">    sess.run(init)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call_model</span><span class="params">(data_x, data_y=None, name=None)</span>:</span></div><div class="line">        <span class="comment"># generate data_y for placeholder while testing</span></div><div class="line">        <span class="keyword">if</span> data_y <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            data_y = np.zeros(data_x.shape[:<span class="number">-1</span>])</div><div class="line">            </div><div class="line">        <span class="keyword">if</span> name == <span class="string">'test'</span>:</div><div class="line">            ret = sess.run(  <span class="comment"># return y_pred</span></div><div class="line">                test_output,</div><div class="line">                feed_dict=classifier.gen_input(data_x, data_y))</div><div class="line">        <span class="keyword">else</span>:  <span class="comment"># name == 'train'</span></div><div class="line">            _, ret = sess.run(  <span class="comment"># return y_pred, loss</span></div><div class="line">                [optimizer, train_output], </div><div class="line">                feed_dict=classifier.gen_input(data_x, data_y))</div><div class="line">        <span class="keyword">return</span> ret</div><div class="line">    </div><div class="line">    epoch = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> epoch &lt; n_epochs:</div><div class="line">        <span class="comment"># draw a figure every 'draw_freq' times</span></div><div class="line">        <span class="keyword">if</span> epoch % draw_freq == <span class="number">0</span>:</div><div class="line">            <span class="comment"># print(train_set_x, train_set_y)</span></div><div class="line">            plot_decision_boundary(</div><div class="line">                <span class="keyword">lambda</span> x: call_model(x)[<span class="number">0</span>], </div><div class="line">                train_set_x, train_set_y)</div><div class="line">        </div><div class="line">        <span class="comment"># print error/cost per epoch</span></div><div class="line">        train_pred, loss = call_model(</div><div class="line">            train_set_x, train_set_y, <span class="string">'train'</span>)</div><div class="line">        train_error = classifier.errors(</div><div class="line">            y_pred=train_pred, y_truth=train_set_y)</div><div class="line">        </div><div class="line">        test_pred = call_model(</div><div class="line">            test_set_x,  test_set_y, <span class="string">'test'</span>)</div><div class="line">        test_error = classifier.errors(</div><div class="line">            y_pred=test_pred, y_truth=test_set_y)</div><div class="line"></div><div class="line">        <span class="keyword">print</span> (<span class="string">"epoch is %d, train error %f, test error %f"</span> % (</div><div class="line">            epoch, train_error, test_error))</div><div class="line">        epoch += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment"># draw a figure at last        </span></div><div class="line">    plot_decision_boundary(</div><div class="line">        <span class="keyword">lambda</span> x: call_model(x)[<span class="number">0</span>], </div><div class="line">        train_set_x, train_set_y)</div><div class="line">    sess.close()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(pred_func, train_set_x, train_set_y)</span>:</span></div><div class="line">    <span class="comment"># Draw figures as Matlab </span></div><div class="line">    x_min, x_max = train_set_x[:, <span class="number">0</span>].min() - <span class="number">.5</span>, train_set_x[:, <span class="number">0</span>].max() + <span class="number">.5</span></div><div class="line">    y_min, y_max = train_set_x[:, <span class="number">1</span>].min() - <span class="number">.5</span>, train_set_x[:, <span class="number">1</span>].max() + <span class="number">.5</span></div><div class="line">    h = <span class="number">0.01</span></div><div class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</div><div class="line">    grid_input = np.c_[xx.ravel(), yy.ravel()].reshape([<span class="number">-1</span>, <span class="number">2</span>])</div><div class="line">    Z = pred_func(grid_input)</div><div class="line">    plt.contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.Spectral)</div><div class="line">    plt.scatter(train_set_x[:, <span class="number">0</span>], train_set_x[:, <span class="number">1</span>], c=train_set_y, cmap=plt.cm.Spectral)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><h3 id="Start-Training"><a href="#Start-Training" class="headerlink" title="Start Training"></a>Start Training</h3><blockquote><p>对于模型选择输入数据（data），设置参数（迭代次数，每N次绘制一次）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pickle</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</div><div class="line">    data = load_data(<span class="string">'moons'</span>, load_directly=<span class="keyword">True</span>)</div><div class="line">    sgd_optimization(data, learning_rate=<span class="number">0.1</span>, n_epochs=<span class="number">100</span>, draw_freq=<span class="number">25</span>)</div></pre></td></tr></table></figure><p><img src="/assets/output_9_0.png" alt="png"></p><pre><code>epoch is 0, train error 0.513333, test error 0.460000epoch is 1, train error 0.513333, test error 0.165000epoch is 2, train error 0.248333, test error 0.160000epoch is 3, train error 0.201667, test error 0.175000epoch is 4, train error 0.215000, test error 0.185000epoch is 5, train error 0.206667, test error 0.185000epoch is 6, train error 0.200000, test error 0.185000epoch is 7, train error 0.200000, test error 0.185000epoch is 8, train error 0.201667, test error 0.190000epoch is 9, train error 0.203333, test error 0.190000epoch is 10, train error 0.198333, test error 0.195000epoch is 11, train error 0.198333, test error 0.195000epoch is 12, train error 0.201667, test error 0.195000epoch is 13, train error 0.201667, test error 0.195000epoch is 14, train error 0.198333, test error 0.190000epoch is 15, train error 0.198333, test error 0.190000epoch is 16, train error 0.198333, test error 0.190000epoch is 17, train error 0.195000, test error 0.190000epoch is 18, train error 0.195000, test error 0.185000epoch is 19, train error 0.195000, test error 0.185000epoch is 20, train error 0.193333, test error 0.180000epoch is 21, train error 0.191667, test error 0.180000epoch is 22, train error 0.191667, test error 0.180000epoch is 23, train error 0.191667, test error 0.180000epoch is 24, train error 0.190000, test error 0.180000</code></pre><p><img src="/assets/output_9_2.png" alt="png"></p><pre><code>epoch is 25, train error 0.191667, test error 0.175000epoch is 26, train error 0.193333, test error 0.170000epoch is 27, train error 0.193333, test error 0.170000epoch is 28, train error 0.191667, test error 0.165000epoch is 29, train error 0.191667, test error 0.165000epoch is 30, train error 0.193333, test error 0.160000epoch is 31, train error 0.193333, test error 0.160000epoch is 32, train error 0.193333, test error 0.160000epoch is 33, train error 0.193333, test error 0.160000epoch is 34, train error 0.193333, test error 0.160000epoch is 35, train error 0.193333, test error 0.160000epoch is 36, train error 0.193333, test error 0.160000epoch is 37, train error 0.195000, test error 0.160000epoch is 38, train error 0.195000, test error 0.160000epoch is 39, train error 0.195000, test error 0.155000epoch is 40, train error 0.195000, test error 0.155000epoch is 41, train error 0.195000, test error 0.155000epoch is 42, train error 0.193333, test error 0.155000epoch is 43, train error 0.190000, test error 0.160000epoch is 44, train error 0.190000, test error 0.160000epoch is 45, train error 0.188333, test error 0.160000epoch is 46, train error 0.188333, test error 0.160000epoch is 47, train error 0.188333, test error 0.155000epoch is 48, train error 0.186667, test error 0.155000epoch is 49, train error 0.186667, test error 0.155000</code></pre><p><img src="/assets/output_9_4.png" alt="png"></p><pre><code>epoch is 50, train error 0.185000, test error 0.165000epoch is 51, train error 0.185000, test error 0.160000epoch is 52, train error 0.190000, test error 0.155000epoch is 53, train error 0.188333, test error 0.155000epoch is 54, train error 0.190000, test error 0.155000epoch is 55, train error 0.190000, test error 0.155000epoch is 56, train error 0.191667, test error 0.160000epoch is 57, train error 0.190000, test error 0.160000epoch is 58, train error 0.191667, test error 0.155000epoch is 59, train error 0.191667, test error 0.160000epoch is 60, train error 0.191667, test error 0.160000epoch is 61, train error 0.190000, test error 0.155000epoch is 62, train error 0.188333, test error 0.155000epoch is 63, train error 0.188333, test error 0.155000epoch is 64, train error 0.188333, test error 0.155000epoch is 65, train error 0.188333, test error 0.155000epoch is 66, train error 0.188333, test error 0.155000epoch is 67, train error 0.188333, test error 0.155000epoch is 68, train error 0.188333, test error 0.155000epoch is 69, train error 0.186667, test error 0.155000epoch is 70, train error 0.185000, test error 0.155000epoch is 71, train error 0.185000, test error 0.155000epoch is 72, train error 0.183333, test error 0.155000epoch is 73, train error 0.181667, test error 0.155000epoch is 74, train error 0.181667, test error 0.155000</code></pre><p><img src="/assets/output_9_6.png" alt="png"></p><pre><code>epoch is 75, train error 0.181667, test error 0.155000epoch is 76, train error 0.183333, test error 0.145000epoch is 77, train error 0.183333, test error 0.145000epoch is 78, train error 0.185000, test error 0.150000epoch is 79, train error 0.185000, test error 0.150000epoch is 80, train error 0.183333, test error 0.150000epoch is 81, train error 0.183333, test error 0.150000epoch is 82, train error 0.183333, test error 0.150000epoch is 83, train error 0.181667, test error 0.150000epoch is 84, train error 0.181667, test error 0.150000epoch is 85, train error 0.181667, test error 0.150000epoch is 86, train error 0.181667, test error 0.150000epoch is 87, train error 0.180000, test error 0.150000epoch is 88, train error 0.180000, test error 0.150000epoch is 89, train error 0.180000, test error 0.150000epoch is 90, train error 0.180000, test error 0.150000epoch is 91, train error 0.178333, test error 0.150000epoch is 92, train error 0.178333, test error 0.150000epoch is 93, train error 0.180000, test error 0.150000epoch is 94, train error 0.176667, test error 0.150000epoch is 95, train error 0.176667, test error 0.145000epoch is 96, train error 0.176667, test error 0.145000epoch is 97, train error 0.176667, test error 0.145000epoch is 98, train error 0.175000, test error 0.145000epoch is 99, train error 0.175000, test error 0.145000</code></pre><p><img src="/assets/output_9_8.png" alt="png"></p><h3 id="Course-Testing"><a href="#Course-Testing" class="headerlink" title="Course Testing"></a>Course Testing</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a, b = [<span class="number">1</span>,<span class="number">2</span>]</div><div class="line">a, b</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">""</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure><h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Step 1: placeholder</span></div><div class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">2</span>], name=<span class="string">'x'</span>)</div><div class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">2</span>], name=<span class="string">'y'</span>)</div><div class="line"></div><div class="line"><span class="comment"># Step 2: network</span></div><div class="line">output = x * y</div><div class="line"></div><div class="line"><span class="comment"># Step 3: prepare data</span></div><div class="line">data_x = np.random.rand(<span class="number">8</span>).reshape([<span class="number">4</span>,<span class="number">2</span>])</div><div class="line">data_y = np.arange(<span class="number">8</span>).reshape([<span class="number">4</span>,<span class="number">2</span>]).astype(np.int32)</div><div class="line"></div><div class="line"><span class="comment"># Step 4: run</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    output = sess.run(</div><div class="line">        [output],</div><div class="line">        feed_dict = &#123;x: data_x, y: data_y&#125;</div><div class="line">    )</div><div class="line">    print(output)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">a = tf.eye(<span class="number">3</span> ,name=<span class="string">'a'</span>)</div><div class="line">b = tf.ones(<span class="number">3</span>, name=<span class="string">'b'</span>)</div><div class="line">c = tf.zeros([<span class="number">5</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>], dtype=tf.int32, name=<span class="string">'c'</span>)</div><div class="line">d = tf.reshape(tf.range(<span class="number">60</span>), [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>], name=<span class="string">'d'</span>)</div><div class="line">e = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>], name=<span class="string">'e'</span>)</div><div class="line">f = tf.matmul(c, d)</div><div class="line">var = [a,b,c,d,e,f]</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">output = sess.run(</div><div class="line">    var,</div><div class="line">    <span class="comment"># feed_dict = &#123;x: np.ones(3)&#125;</span></div><div class="line">)</div><div class="line"></div><div class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> zip(var, output):</div><div class="line">    print(k)</div><div class="line">    print(v)</div><div class="line">    print()</div><div class="line"></div><div class="line">sess.close()</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;View on Pages (Directly click the title)&lt;/li&gt;
&lt;li&gt;View on &lt;a href=&quot;/assets/CDTensorflow.html&quot;&gt;Jupyter Notebook&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;Download &lt;a href=&quot;/assets/CDTensorflow.ipynb&quot;&gt;Jupyter Notebook&lt;/a&gt;  &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="tensorflow" scheme="https://ai-fundamental.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>The knowledge graph of the second course part: Knowledge</title>
    <link href="https://ai-fundamental.github.io/2017/The%20knowledge%20graph%20of%20the%20second%20course%20part%20Knowledge/"/>
    <id>https://ai-fundamental.github.io/2017/The knowledge graph of the second course part Knowledge/</id>
    <published>2017-11-20T08:00:00.000Z</published>
    <updated>2018-11-29T03:17:19.254Z</updated>
    
    <content type="html"><![CDATA[<p>click <a href="/assets/Knowledge.svg">here</a> for viewing <a href="/assets/Knowledge.svg">the whole knowledge graph</a>.</p><p><a href="/assets/Knowledge.png"><img src="/assets/Knowledge.png" alt="TestSVG"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;click &lt;a href=&quot;/assets/Knowledge.svg&quot;&gt;here&lt;/a&gt; for viewing &lt;a href=&quot;/assets/Knowledge.svg&quot;&gt;the whole knowledge graph&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=
      
    
    </summary>
    
    
      <category term="knowledge graph" scheme="https://ai-fundamental.github.io/tags/knowledge-graph/"/>
    
  </entry>
  
  <entry>
    <title>Homework 3 on Knowledge</title>
    <link href="https://ai-fundamental.github.io/2017/Homework%203%20on%20Knowledge/"/>
    <id>https://ai-fundamental.github.io/2017/Homework 3 on Knowledge/</id>
    <published>2017-11-13T08:00:00.000Z</published>
    <updated>2017-11-13T03:28:30.705Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h3><p>This assignment should be finished with <code>Prolog</code>.</p><h3 id="Question-I"><a href="#Question-I" class="headerlink" title="Question I"></a>Question I</h3><blockquote><p>Given a set, list all its elements.<br>给出一个集合，列出其所有元素     </p></blockquote><figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* using MS Windows GUN prolog（gprolog）</span></div><div class="line"><span class="comment">query ：mymember(X,[1,2,3,4,5]). */</span></div><div class="line"></div><div class="line">mymember(<span class="symbol">X</span>,[<span class="symbol">_</span>|<span class="symbol">Y</span>]):- mymember(<span class="symbol">X</span>,<span class="symbol">Y</span>).</div><div class="line">mymember(<span class="symbol">X</span>,[<span class="symbol">X</span>|<span class="symbol">_</span>]).</div></pre></td></tr></table></figure><p>(Thanks to Shiyu Wu)</p><h3 id="Question-II"><a href="#Question-II" class="headerlink" title="Question II"></a>Question II</h3><blockquote><p>Given a set, list all its subsets.<br>给出一个集合，列出其所有子集     </p></blockquote><figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* gprolog</span></div><div class="line"><span class="comment">Supposing the subset is in order.</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">EXAMPLE:</span></div><div class="line"><span class="comment">| ?- subset([2,3],[1,2,3]).</span></div><div class="line"><span class="comment">true ? </span></div><div class="line"><span class="comment">yes </span></div><div class="line"><span class="comment">| ?- subset(X,[1,2,3]).</span></div><div class="line"><span class="comment">X = [1,2,3] ? ;</span></div><div class="line"><span class="comment">X = [1,2] ? ;</span></div><div class="line"><span class="comment">X = [1,3] ? ;</span></div><div class="line"><span class="comment">X = [1] ? ;</span></div><div class="line"><span class="comment">X = [2,3] ? ;</span></div><div class="line"><span class="comment">X = [2] ? ;</span></div><div class="line"><span class="comment">X = [3] ? ;</span></div><div class="line"><span class="comment">X = [] ? ;</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line">subset([],[]).</div><div class="line">subset([<span class="symbol">A</span>|<span class="symbol">B</span>],[<span class="symbol">A</span>|<span class="symbol">C</span>]):- subset(<span class="symbol">B</span>,<span class="symbol">C</span>).</div><div class="line">subset(<span class="symbol">A</span>,[<span class="symbol">_</span>|<span class="symbol">B</span>]):- subset(<span class="symbol">A</span>,<span class="symbol">B</span>).</div></pre></td></tr></table></figure><p>(Thanks to Pan Shi)</p><h3 id="Related-Prolog-grammar-brief-explanation"><a href="#Related-Prolog-grammar-brief-explanation" class="headerlink" title="Related Prolog grammar: brief explanation"></a>Related Prolog grammar: brief explanation</h3><p><strong> 匿名变量 <code>_</code> </strong><br>prolog中，变量由大写字母或下划线 <code>_</code> 开头。<br>匿名变量 <code>_</code> 不会绑定为任何值，所以也不要求它所出现的位置的值必须相同。    </p><p><strong> 列表表达 <code>|</code> </strong><br>Prolog提供了把表头项目以及除去表头项目后剩下的列表分离的方法。<br>其次，Prolog强大的递归功能可以方便地访问除去表头项目后的列表。<br>如：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[X | Y]</div></pre></td></tr></table></figure></p><p>使用此列表可以与任意的列表匹配，匹配成功后，X绑定为列表的第一个项目的值，<br>我们称之为表头（head）。而Y则绑定为剩下的列表，我们称之为表尾（tail)，举例说明：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">?- [<span class="symbol">H</span>|<span class="symbol">T</span>] = [apple, broccoli, refrigerator].</div><div class="line"><span class="symbol">H</span> = apple </div><div class="line"><span class="symbol">T</span> = [broccoli, refrigerator] </div><div class="line"></div><div class="line">?- [<span class="symbol">H</span>|<span class="symbol">T</span>] = [a, b, c, d, e].</div><div class="line"><span class="symbol">H</span> = a</div><div class="line"><span class="symbol">T</span> = [b, c, d, e] </div><div class="line"></div><div class="line">?- [<span class="symbol">H</span>|<span class="symbol">T</span>] = [apples, bananas].</div><div class="line"><span class="symbol">H</span> = apples </div><div class="line"><span class="symbol">T</span> = [bananas]</div><div class="line"></div><div class="line">?- [<span class="symbol">H</span>|<span class="symbol">T</span>] = [a, [b,c,d]]. <span class="comment">/* 这个例子中的第一层列表有两个项目。*/</span></div><div class="line"><span class="symbol">H</span> = a </div><div class="line"><span class="symbol">T</span> = [[b, c, d]] </div><div class="line"></div><div class="line">?- [<span class="symbol">H</span>|<span class="symbol">T</span>] = [apples]. <span class="comment">/*列表中只有一个项目的情况 */</span></div><div class="line"><span class="symbol">H</span> = apples</div><div class="line"><span class="symbol">T</span> = []</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Foreword&quot;&gt;&lt;a href=&quot;#Foreword&quot; class=&quot;headerlink&quot; title=&quot;Foreword&quot;&gt;&lt;/a&gt;Foreword&lt;/h3&gt;&lt;p&gt;This assignment should be finished with &lt;code&gt;
      
    
    </summary>
    
    
      <category term="homework" scheme="https://ai-fundamental.github.io/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>Homework 2 on Knowledge</title>
    <link href="https://ai-fundamental.github.io/2017/Homework%202%20on%20Knowledge/"/>
    <id>https://ai-fundamental.github.io/2017/Homework 2 on Knowledge/</id>
    <published>2017-10-23T08:00:00.000Z</published>
    <updated>2017-10-23T05:35:23.099Z</updated>
    
    <content type="html"><![CDATA[<p>We give the detailed answer to Exercise 7.17 in the book of AIMA.</p><p><strong>7.17</strong> A propositional 2-CNF expression is a conjunction of clauses, each containing <em>exactly</em> 2 literals, e.g.,<br>$$(A \lor B) \land (\lnot A \lor C) \land (\lnot B \lor D) \land (\lnot C \lor G) \land (\lnot D \lor G)$$</p><p><strong>a</strong>. Prove using resolution that the above sentence entails G.<br><strong>b</strong>. Two clauses are <em>semantically distinct</em> if they are not logically equivalent. How many semantically distinct 2-CNF clauses can be constructed from n proposition symbols?<br><strong>c</strong>. Using your answer to (b), prove that propositional resolution always terminates in time polynomial in n given a 2-CNF sentence containing no more than n distinct symbols.<br><strong>d</strong>. Explain why your argument in (c) does not apply to 3-CNF. </p><p>详细的证明请见 <a href="/assets/2015K8009929049_冯吕_07.pdf">pdf</a>.<br>(Thanks to Lv Feng.)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We give the detailed answer to Exercise 7.17 in the book of AIMA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7.17&lt;/strong&gt; A propositional 2-CNF expression is a conj
      
    
    </summary>
    
    
      <category term="homework" scheme="https://ai-fundamental.github.io/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>Homework 1 on Knowledge</title>
    <link href="https://ai-fundamental.github.io/2017/Homework%201%20on%20Knowledge/"/>
    <id>https://ai-fundamental.github.io/2017/Homework 1 on Knowledge/</id>
    <published>2017-10-14T08:00:00.000Z</published>
    <updated>2017-10-18T05:36:09.169Z</updated>
    
    <content type="html"><![CDATA[<p>陆钟万先生编写的《面向计算机科学的数理逻辑》是数理逻辑方面的经典教材。<br>本次作业，利用课本讲授的包含11条推演规则的命题逻辑系统，以形式推演的方式证明了课本中的</p><ul><li>定理2.6.4</li><li>定理2.6.9</li></ul><p>详细的证明请见 <a href="/assets/2015K8009929046_王荣庆_06.pdf">pdf</a>.<br>(Thanks to Rongqing Wang.)</p><p>向陆钟万先生致敬！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;陆钟万先生编写的《面向计算机科学的数理逻辑》是数理逻辑方面的经典教材。&lt;br&gt;本次作业，利用课本讲授的包含11条推演规则的命题逻辑系统，以形式推演的方式证明了课本中的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定理2.6.4&lt;/li&gt;
&lt;li&gt;定理2.6.9&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;详
      
    
    </summary>
    
    
      <category term="homework" scheme="https://ai-fundamental.github.io/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>The knowledge graph of the first course part: Search</title>
    <link href="https://ai-fundamental.github.io/2017/The%20knowledge%20graph%20of%20the%20first%20course%20part%20Search/"/>
    <id>https://ai-fundamental.github.io/2017/The knowledge graph of the first course part Search/</id>
    <published>2017-09-28T08:00:00.000Z</published>
    <updated>2018-11-29T03:18:02.478Z</updated>
    
    <content type="html"><![CDATA[<p>click <a href="/assets/Search.km.svg">here</a> for viewing <a href="/assets/Search.km.svg">the whole knowledge graph</a>.</p><p><a href="/assets/Search.km.png"><img src="/assets/Search.km.png" alt="TestSVG"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;click &lt;a href=&quot;/assets/Search.km.svg&quot;&gt;here&lt;/a&gt; for viewing &lt;a href=&quot;/assets/Search.km.svg&quot;&gt;the whole knowledge graph&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=
      
    
    </summary>
    
    
      <category term="knowledge graph" scheme="https://ai-fundamental.github.io/tags/knowledge-graph/"/>
    
  </entry>
  
  <entry>
    <title>Homework 3 on Search</title>
    <link href="https://ai-fundamental.github.io/2017/Homework%203%20on%20Search/"/>
    <id>https://ai-fundamental.github.io/2017/Homework 3 on Search/</id>
    <published>2017-09-16T08:00:00.000Z</published>
    <updated>2017-10-10T07:41:04.516Z</updated>
    
    <content type="html"><![CDATA[<p>We provide a formal proof on the correctness of NegaScout, which cannot be found in other places of Web.</p><p>We also find that we can also perform the scout step even for the first child of a node. It means that the constraint on the “first action” (see Line 6 of the algorithm in the <a href="/assets/2015K8009929046_王荣庆_04.pdf">pdf</a>) is <strong>NOT</strong> necessary. It is only necessary when $\alpha = -\infty $.</p><p>Please check the following <a href="/assets/2015K8009929046_王荣庆_04.pdf">pdf</a>.<br>(Thanks to Rongqing Wang.)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We provide a formal proof on the correctness of NegaScout, which cannot be found in other places of Web.&lt;/p&gt;
&lt;p&gt;We also find that we can 
      
    
    </summary>
    
    
      <category term="homework" scheme="https://ai-fundamental.github.io/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>Homework 2 on Search</title>
    <link href="https://ai-fundamental.github.io/2017/Homework%202%20on%20Search/"/>
    <id>https://ai-fundamental.github.io/2017/Homework 2 on Search/</id>
    <published>2017-09-13T08:00:00.000Z</published>
    <updated>2017-10-10T08:22:03.333Z</updated>
    
    <content type="html"><![CDATA[<p>We provide a formal proof on the following properties</p><ul><li>If a heuristic is consistent, it must be admissible. We also give an example heuristic which is admissible, but not consistent.</li><li><code>A*</code> of graph search is <strong>NOT</strong> optimal with <strong>admissible heuristic</strong>.</li></ul><p>For the second property, the course book only says that “<code>A*</code> of graph search is optimal with <strong>consistent heuristic</strong>”. Some students are wondering whether the conclusion that   </p><p><strong> A* of graph search is optimal with admissible heuristic </strong></p><p>is true or not (see the <a href="https://cs.stackexchange.com/questions/23351/optimality-of-a?rq=1" target="_blank" rel="external">page</a>). Here, we show this conclusion is wrong. The key point is that the frontier set in graph search include both the visited and unvisited nodes.</p><p>Please check the following <a href="/assets/main.pdf">pdf</a>.<br>(Thanks to Xingcheng Ruan.)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We provide a formal proof on the following properties&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If a heuristic is consistent, it must be admissible. We also give an e
      
    
    </summary>
    
    
      <category term="homework" scheme="https://ai-fundamental.github.io/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>Homework 1 on Search</title>
    <link href="https://ai-fundamental.github.io/2017/Homework%201%20on%20Search/"/>
    <id>https://ai-fundamental.github.io/2017/Homework 1 on Search/</id>
    <published>2017-09-09T08:00:00.000Z</published>
    <updated>2017-10-10T07:37:52.397Z</updated>
    
    <content type="html"><![CDATA[<p>We provide a formal proof on</p><ul><li>The graph separation property in graph search</li><li>The optimality on uniform-cost search</li></ul><p>Please check the following <a href="/assets/2015K8009929017_朱钦霖_01.pdf">pdf</a>.<br>(Thanks to Qinlin Zhu.)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;We provide a formal proof on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The graph separation property in graph search&lt;/li&gt;
&lt;li&gt;The optimality on uniform-cost search&lt;/l
      
    
    </summary>
    
    
      <category term="homework" scheme="https://ai-fundamental.github.io/tags/homework/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to AI Fundamental course at UCAS</title>
    <link href="https://ai-fundamental.github.io/2017/Welcome%20to%20AI%20Fundamental%20course%20at%20UCAS/"/>
    <id>https://ai-fundamental.github.io/2017/Welcome to AI Fundamental course at UCAS/</id>
    <published>2017-09-01T08:00:00.000Z</published>
    <updated>2017-10-10T07:38:14.875Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to AI Fundamental course at <a href="http://www.ucas.ac.cn/" target="_blank" rel="external">UCAS</a></p><p>Thank you for your interest in our materials developed for AI Fundamental course (2017) at University of Chinese Academy of Sciences. At this web site, you will find the following:</p><ul><li>Some lecture slides (mainly for the third part of this course)</li><li>Detailed analysis on some key questions, which are not well addressed in the course book. These questions are usually used as the homework for the students in the course.</li></ul><a id="more"></a><p>The whole course includes the following three parts:<br>Part 1: Search<br>Part 2: Knowledge<br>Part 3: Learning</p><p>The materials for the first two parts are mainly based on <a href="http://aima.cs.berkeley.edu/" target="_blank" rel="external">AIMA</a> (CS 188 at Berkeley) and the <a href="https://www.coursera.org/learn/rengong-zhineng" target="_blank" rel="external">AI course at Taiwan University</a>. The materials for the last part are mainly developed by the instructor himself.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to AI Fundamental course at &lt;a href=&quot;http://www.ucas.ac.cn/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;UCAS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thank you for your interest in our materials developed for AI Fundamental course (2017) at University of Chinese Academy of Sciences. At this web site, you will find the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some lecture slides (mainly for the third part of this course)&lt;/li&gt;
&lt;li&gt;Detailed analysis on some key questions, which are not well addressed in the course book. These questions are usually used as the homework for the students in the course.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
